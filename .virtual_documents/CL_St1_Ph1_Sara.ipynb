























import pandas as pd
import os
import sys
import nltk
from nltk.tokenize import word_tokenize
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt





input_directory = 'cl_st1_ph1_sara'
output_directory = 'cl_st1_ph1_sara'
id = 'short_stories'
dataset_filename_1 = f"{id}_full"
dataset_filename_2 = f"{id}_per_author"
dataset_filename_3 = f"{id}_sample"





# Check if the output directory already exists. If it does, do nothing. If it doesn't exist, create it.
if os.path.exists(output_directory):
    print('Output directory already exists.')
else:
    try:
        os.makedirs(output_directory)
        print('Output directory successfully created.')
    except OSError as e:
        print('Failed to create the directory:', e)
        sys.exit(1)





df_short_stories = pd.read_excel(f"{input_directory}/4000-Stories-with-sentiment-analysis.xlsx", engine='openpyxl')


df_short_stories





df_short_stories.loc[3915, 'author']


df_short_stories.loc[3915, 'story']


df_short_stories.loc[3915, 'processed_text']








nltk.download('punkt', quiet=True)
nltk.download('punkt_tab', quiet=True)

df_short_stories_2 = df_short_stories.copy()
df_short_stories_2['word_count_nltk'] = df_short_stories_2['story'].apply(
    lambda s: sum(1 for tok in word_tokenize(s) if tok.isalnum()) if isinstance(s, str) else 0)
df_short_stories_2


df_short_stories_2_columns = df_short_stories_2.columns.tolist()
print(df_short_stories_2_columns)


# Reorder the columns to place the `word_count_nltk` column after the `word_count` column
df_short_stories_2 = df_short_stories_2[['Unnamed: 0', 'url', 'length', 'title', 'text_no', 'author', 'story', 'valence', 'arousal', 'dominance', 'text_id', 'processed_text', 'word_count', 'word_count_nltk'] + [col for col in df_short_stories_2.columns if col not in ['Unnamed: 0', 'url', 'length', 'title', 'text_no', 'author', 'story', 'valence', 'arousal', 'dominance', 'text_id', 'processed_text', 'word_count', 'word_count_nltk']]]


df_short_stories_2





df_short_stories_2.to_json(f"{output_directory}/{dataset_filename_1}.jsonl", orient='records', lines=True)








df_stories_per_author = (
    df_short_stories_2
    .groupby('author', dropna=False)
    .size()
    .reset_index(name='story_count')
    .sort_values(['story_count', 'author'], ascending=[False, True])
    .reset_index(drop=True)
)





df_stories_per_author['balanced_story_count'] = (df_stories_per_author.index + 1) * df_stories_per_author['story_count']
df_stories_per_author





df_stories_per_author.to_json(f"{output_directory}/{dataset_filename_2}.jsonl", orient='records', lines=True)


df_stories_per_author.to_excel(f"{output_directory}/{dataset_filename_2}.xlsx", index=False)











authors_from_79 = df_stories_per_author.iloc[79:]['author'].tolist()
authors_from_79





df_short_stories_sample = df_short_stories_2[~df_short_stories_2['author'].isin(authors_from_79)].copy()
df_short_stories_sample








df_counts = df_short_stories_sample['author'].value_counts()
eligible_authors = df_counts[df_counts >= 13].index

df_short_stories_sample_balanced = (
    df_short_stories_sample[df_short_stories_sample['author'].isin(eligible_authors)]
    .groupby('author', group_keys=False)
    .sample(n=13, random_state=42)
    .reset_index(drop=True)
)


df_short_stories_sample_balanced





df_short_stories_sample_balanced.to_json(f"{output_directory}/{dataset_filename_3}.jsonl", orient='records', lines=True)


df_short_stories_sample_balanced.to_excel(f"{output_directory}/{dataset_filename_3}.xlsx", index=False)





# Select the series
series = df_short_stories_sample_balanced['word_count_nltk'].dropna()

# Summary stats
mean_val = series.mean()
std_val = series.std(ddof=1)  # sample standard deviation
desc = series.describe()  # count, mean, std, min, 25%, 50%, 75%, max

print("Descriptive statistics for word_count_nltk:")
print(f"- Count: {int(desc['count'])}")
print(f"- Mean: {mean_val:.2f}")
print(f"- Std (sample): {std_val:.2f}")
print(f"- Min: {int(desc['min'])}")
print(f"- Q1 (25%): {desc['25%']:.2f}")
print(f"- Median (50%): {desc['50%']:.2f}")
print(f"- Q3 (75%): {desc['75%']:.2f}")
print(f"- Max: {int(desc['max'])}")

# IQR-based outlier detection
q1, q3 = series.quantile([0.25, 0.75])
iqr = q3 - q1
lower_fence = q1 - 1.5 * iqr
upper_fence = q3 + 1.5 * iqr

outlier_mask = (series < lower_fence) | (series > upper_fence)
outliers = (
    df_short_stories_sample_balanced.loc[outlier_mask, ['author', 'title', 'word_count_nltk']]
    .sort_values('word_count_nltk')
    .reset_index(drop=True)
)

print(f"\nIQR outlier thresholds:")
print(f"- Lower fence: {lower_fence:.2f}")
print(f"- Upper fence: {upper_fence:.2f}")
print(f"Outliers found: {len(outliers)}")
display(outliers.head(10))  # show a preview

# Box plot
plt.figure(figsize=(9, 1.8))
sns.boxplot(x=series, color='steelblue')
plt.title('Box plot of word_count_nltk (df_short_stories_sample_balanced)')
plt.xlabel('Tokens')
plt.tight_layout()
plt.show()
